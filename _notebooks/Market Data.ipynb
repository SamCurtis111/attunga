{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "median-letter",
   "metadata": {},
   "source": [
    "# Market Data\n",
    "> Updated NEM data\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considered-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "from datetime import datetime, date, timedelta\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from IPython.display import display_html, HTML\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disabled-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "\n",
    "# Global variables #\n",
    "current_date = datetime.date(datetime.now())\n",
    "yesterday = current_date - timedelta(days=1)\n",
    "states = [\"QLD\",\"NSW\",\"VIC\",\"SA\",\"TAS\",\"ACT\",\"WA\",\"NT\"]\n",
    "capital_cities = [\"Brisbane\",\"Sydney\",\"Melbourne\",\"Adelaide\",\"Hobart\",\"Canberra\",\"Perth\",\"Darwin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pacific-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran:  2021-02-04 12:29:18\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print('Last ran: ', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-lobby",
   "metadata": {},
   "source": [
    "# NEM Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equivalent-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "base_url = \"http://www.nemweb.com.au\"\n",
    "section = \"Reports/CURRENT\"\n",
    "\n",
    "\n",
    "start_date=datetime.today().strftime(\"%Y%m%d\")\n",
    "start_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "\n",
    "end_date='30001225'\n",
    "end_date = datetime.strptime(end_date, '%Y%m%d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CurrentDataset = namedtuple(\"NemwebCurrentFile\",\n",
    "                            [\"dataset_name\",\n",
    "                             \"nemfile_pattern\",\n",
    "                             \"datetime_format\",\n",
    "                             \"datetime_column\",\n",
    "                             \"tables\"])\n",
    "\n",
    "DATASETS = {\n",
    "    \"pd7day_gpg\": CurrentDataset(\n",
    "        dataset_name=\"PD7DAY\",\n",
    "        nemfile_pattern=\"PUBLIC_PD7DAY_GPG_([0-9]{14})_[0-9]{16}.zip\",\n",
    "        datetime_format=\"%Y%m%d%H%M%S\",\n",
    "        datetime_column=\"INTERVAL_DATETIME\",\n",
    "        tables=['GPG_PRICESOLUTION'])   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dataset = DATASETS['pd7day_gpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposed-criticism",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [00:00, 13291.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "class ZipFileStreamer(ZipFile):\n",
    "    \"\"\"ZipFile subclass, with method to extract ZipFile as byte stream to memory\"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"Initialises ZipFile object, and adds member_count attribute\"\"\"\n",
    "        ZipFile.__init__(self, filename)\n",
    "        self.member_count = len(self.filelist)\n",
    "\n",
    "    def extract_stream(self, member):\n",
    "        \"\"\"Extract a member from the archive as a byte stream or string steam, using\n",
    "        its full name. 'member' may be a filename or a ZipInfo object. \"\"\"\n",
    "        return BytesIO(self.read(member))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "page = requests.get(\"{0}/{1}/{2}/\".format(base_url,\n",
    "                                          section,\n",
    "                                          \"PD7DAY\"))\n",
    "\n",
    "regex = re.compile(\"/{0}/{1}/{2}\".format(section,\n",
    "                                         \"PD7DAY\",\n",
    "                                         \"PUBLIC_PD7DAY_GPG_([0-9]{14})_[0-9]{16}.zip\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df = []     # initi empty dict to store results\n",
    "results_table = []\n",
    "\n",
    "\n",
    "\n",
    "for match in tqdm(regex.finditer(page.text)):\n",
    "    file_datetime = datetime.strptime(match.group(1), dataset.datetime_format)\n",
    "    final_match = match\n",
    "    \n",
    "\n",
    "# add function for timing code\n",
    "import pprofile\n",
    "profiler = pprofile.Profile()\n",
    "    \n",
    "''' seperate the for loop so that we only get the most recent file'''\n",
    "''' only do this for the pd7day file '''\n",
    "''' TO DO: re-attach this so that the daily reports show how the PD7Day has changed throughout the day '''\n",
    "with profiler:\n",
    "    if end_date > file_datetime > start_date:\n",
    "        ''' unpack the download function here'''\n",
    "        response = requests.get(\"{0}{1}\".format(base_url, final_match.group(0)))\n",
    "        zip_bytes = BytesIO(response.content)\n",
    "        ''' function then jumps to nemfile_reader.nemzip_reader(zip_bytes) '''\n",
    "        with ZipFileStreamer(zip_bytes) as zipfile:\n",
    "            if zipfile.member_count == 1:\n",
    "                filename = zipfile.namelist()[0]    # extracts the name of the csv file we want\n",
    "                nemfile_object = zipfile.extract_stream(filename)      # io.BytesIO object\n",
    "        ''' nemfile_object is then passed into nemfile_reader.nemfile_reader(nemfile_object) '''\n",
    "        table_dict = {}\n",
    "        table_dict = nemfile_object.readlines()\n",
    "                        \n",
    "    \n",
    "\n",
    "# Use list comprehension to extract price_solution\n",
    "zz = [x for x in table_dict if b'PRICESOLUTION' in x]\n",
    "\n",
    "\n",
    "# Convert the above list of bytest (zz) to a pd.DF \n",
    "table_dict = {}\n",
    "for line in zz:\n",
    "    rows = line.decode().split(',')\n",
    "    table = \"{0}_{1}\".format(rows[1], rows[2])\n",
    "    \n",
    "    #new table\n",
    "    if rows[0] == \"I\":\n",
    "        table_dict[table] = line\n",
    "\n",
    "    #append data to each table\n",
    "    elif rows[0] == \"D\":\n",
    "        table_dict[table] += line\n",
    "        \n",
    "price_dict = {table:pd.read_csv(BytesIO(table_dict[table]))}    # convert list of bytes to dict\n",
    "price_frame = price_dict['GPG_PRICESOLUTION']\n",
    "price_frame['INTERVAL_DATETIME'] = pd.to_datetime(price_frame['INTERVAL_DATETIME'], format='%Y/%m/%d %H:%M:%S')\n",
    "price_frame['INTERVAL_DATE'] = price_frame['INTERVAL_DATETIME'].dt.date     # extract date only for use in pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-logic",
   "metadata": {},
   "source": [
    "## Averaged Daily Prices\n",
    "Note that current day and 8 day ahead are partial days, do not use these figures for pricing / modelling  \n",
    "Click on individual states in chart to add / remove them for closer inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "present-ancient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONID        NSW1   QLD1      SA1    TAS1    VIC1         RUN_DATETIME\n",
      "INTERVAL_DATE                                                            \n",
      "2021-02-04     39.61  49.92    38.16   35.82   36.55  2021/02/04 07:30:00\n",
      "2021-02-05     56.36  56.59    -7.37   30.82   29.72  2021/02/04 07:30:00\n",
      "2021-02-06     43.24  54.70  -297.64  213.48 -318.70  2021/02/04 07:30:00\n",
      "2021-02-07     30.07  82.02   222.64   40.65    9.39  2021/02/04 07:30:00\n",
      "2021-02-08     45.02  62.31  -117.84  308.23   10.38  2021/02/04 07:30:00\n",
      "2021-02-09     42.43  44.08   558.90  114.46   26.62  2021/02/04 07:30:00\n",
      "2021-02-10     35.17  35.03  2273.05   73.42   30.55  2021/02/04 07:30:00\n",
      "2021-02-11     30.41  29.83    31.87   34.83   27.23  2021/02/04 07:30:00\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "''' create a pivot table '''\n",
    "''' IMPORTANT: should probably remove first and last data aggregations because they are incomplete days '''\n",
    "price_pivot = pd.pivot_table(price_frame, values='RRP', index=['INTERVAL_DATE', 'REGIONID', 'RUN_DATETIME'], aggfunc=np.mean)\n",
    "\n",
    "price_pivot = price_pivot.reset_index()\n",
    "price_pivot = price_pivot.pivot(index='INTERVAL_DATE', columns='REGIONID', values='RRP')\n",
    "price_pivot = price_pivot.round(2)\n",
    "price_pivot['RUN_DATETIME'] = price_frame['RUN_DATETIME'][0]\n",
    "print(price_pivot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
